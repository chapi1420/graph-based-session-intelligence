{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea5a04a",
   "metadata": {},
   "source": [
    "\n",
    "## graph based session intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89bb844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/nahomnadew/.cache/kagglehub/datasets/retailrocket/ecommerce-dataset/versions/2\n"
     ]
    }
   ],
   "source": [
    "from genericpath import exists\n",
    "import kagglehub\n",
    "\n",
    "if not exists(\"data/.kaggle/events.csv\"):\n",
    "    path = kagglehub.dataset_download(\"retailrocket/ecommerce-dataset\")\n",
    "else:\n",
    "    path = \"data/.kaggle/events.csv\"\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e8665a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial parameters\n",
    "\n",
    "DATA_PATH = \"data/kaggle/events.csv\"  \n",
    "SAMPLE_MAX_SESSIONS = 200000  \n",
    "MIN_SESSION_LENGTH = 2  \n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_AUTH = (\"neo4j\", \"nahi1420\")  \n",
    "BATCH_SIZE = 5000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 2.3.3\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"pandas\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fcf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, this may take a while for the full RetailRocket dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9618/3092328460.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(DATA_PATH, parse_dates=['timestamp'], low_memory=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 2756101\n",
      "Columns: ['timestamp', 'visitorid', 'event', 'itemid', 'transactionid']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433221332117</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433224214164</td>\n",
       "      <td>992329</td>\n",
       "      <td>view</td>\n",
       "      <td>248676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433221999827</td>\n",
       "      <td>111016</td>\n",
       "      <td>view</td>\n",
       "      <td>318965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433221955914</td>\n",
       "      <td>483717</td>\n",
       "      <td>view</td>\n",
       "      <td>253185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433221337106</td>\n",
       "      <td>951259</td>\n",
       "      <td>view</td>\n",
       "      <td>367447</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp  visitorid event  itemid  transactionid\n",
       "0  1433221332117     257597  view  355908            NaN\n",
       "1  1433224214164     992329  view  248676            NaN\n",
       "2  1433221999827     111016  view  318965            NaN\n",
       "3  1433221955914     483717  view  253185            NaN\n",
       "4  1433221337106     951259  view  367447            NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset \n",
    "assert os.path.exists(DATA_PATH), f\"Data file not found: {DATA_PATH}\"\n",
    "print(\"Loading, this may take a while for the full RetailRocket dataset...\")\n",
    "\n",
    "# The RetailRocket 'events.csv' has columns similar to: sessionId, itemId, eventType, timestamp\n",
    "usecols = None  \n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e887c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed columns mapping: {'visitorid': 'session_id', 'itemid': 'event_id', 'event': 'event_type', 'timestamp': 'timestamp'}\n",
      "After filtering short sessions: 406020 unique sessions, 1754541 rows\n"
     ]
    }
   ],
   "source": [
    "# Corrected RetailRocket column mapping\n",
    "cols = [c.lower() for c in df.columns]\n",
    "colmap = {}\n",
    "if 'visitorid' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='visitorid'][0]] = 'session_id'\n",
    "if 'itemid' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='itemid'][0]] = 'event_id'\n",
    "if 'event' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='event'][0]] = 'event_type'\n",
    "if 'timestamp' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='timestamp'][0]] = 'timestamp'\n",
    "\n",
    "df = df.rename(columns=colmap)\n",
    "print('Renamed columns mapping:', colmap)\n",
    "\n",
    "assert 'session_id' in df.columns and 'event_id' in df.columns and 'timestamp' in df.columns\n",
    "\n",
    "# Add step_index per session (order by timestamp)\n",
    "df = df.sort_values(['session_id','timestamp'])\n",
    "df['step_index'] = df.groupby('session_id').cumcount() + 1\n",
    "\n",
    "# Filter out sessions that are too short\n",
    "session_lengths = df.groupby('session_id').size()\n",
    "valid_sessions = session_lengths[session_lengths >= MIN_SESSION_LENGTH].index\n",
    "df = df[df['session_id'].isin(valid_sessions)].copy()\n",
    "print(\"After filtering short sessions:\", df['session_id'].nunique(), \"unique sessions,\", len(df), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c608219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled sessions: 200000 rows: 869349\n"
     ]
    }
   ],
   "source": [
    "# pick the first N sessions (keeps temporal order inside sessions)\n",
    "if SAMPLE_MAX_SESSIONS is not None:\n",
    "    unique_sess = df['session_id'].drop_duplicates().iloc[:SAMPLE_MAX_SESSIONS]\n",
    "    df = df[df['session_id'].isin(unique_sess)].copy()\n",
    "    print(\"Sampled sessions:\", df['session_id'].nunique(), \"rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe88d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample to data/sampled_sessions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save a sampled CSV for quick checks \n",
    "sample_out = \"data/sampled_sessions.csv\"\n",
    "df.to_csv(sample_out, index=False)\n",
    "print(\"Saved sample to\", sample_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96663298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9618/3748043458.py:1: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n"
     ]
    }
   ],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97860eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Range Check ---\n",
      "First event timestamp: 2015-05-03 03:00:04.384000\n",
      "Last event timestamp: 2015-09-18 02:57:40.810000\n"
     ]
    }
   ],
   "source": [
    "#Check Data Range and Determine Cutoff \n",
    "print(\"\\n--- Data Range Check ---\")\n",
    "min_ts = df['timestamp'].min()\n",
    "max_ts = df['timestamp'].max()\n",
    "print(f\"First event timestamp: {min_ts}\")\n",
    "print(f\"Last event timestamp: {max_ts}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52cd1fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Cutoff Date: 2015-09-11 02:57:40.810000\n",
      "Train sessions: 193183 (Rows: 834594)\n",
      "Test sessions: 11074 (Rows: 34755)\n"
     ]
    }
   ],
   "source": [
    "total_duration = df['timestamp'].max() - df['timestamp'].min()\n",
    "SEVEN_DAYS_DELTA = pd.Timedelta(days=7)\n",
    "SPLIT_CUTOFF_DATE = df['timestamp'].max() - SEVEN_DAYS_DELTA\n",
    "\n",
    "df_train = df[df['timestamp'] < SPLIT_CUTOFF_DATE].copy()\n",
    "df_test = df[df['timestamp'] >= SPLIT_CUTOFF_DATE].copy()\n",
    "\n",
    "\n",
    "print(f\"Split Cutoff Date: {SPLIT_CUTOFF_DATE}\")\n",
    "print(f\"Train sessions: {df_train['session_id'].nunique()} (Rows: {len(df_train)})\")\n",
    "print(f\"Test sessions: {df_test['session_id'].nunique()} (Rows: {len(df_test)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd87aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j connection helper\n",
    "def get_driver(uri=NEO4J_URI, auth=NEO4J_AUTH):\n",
    "    return GraphDatabase.driver(uri, auth=auth, encrypted=False)\n",
    "\n",
    "def create_constraints(driver):\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(lambda tx: tx.run(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Event) REQUIRE e.id IS UNIQUE\"\"\"))\n",
    "        session.execute_write(lambda tx: tx.run(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (s:Session) REQUIRE s.id IS UNIQUE\"\"\"))\n",
    "    print('Constraints ensured.')\n",
    "\n",
    "def build_graph_from_df(df, driver, batch_size=BATCH_SIZE):\n",
    "    # df must contain: session_id, timestamp, step_index, event_id, event_type (optional)\n",
    "    create_constraints(driver)\n",
    "    sessions = df['session_id'].drop_duplicates().tolist()\n",
    "    print('Building nodes for', len(sessions), 'sessions (batched)')\n",
    "    with driver.session() as session:\n",
    "        for i in tqdm(range(0, len(sessions), batch_size), desc='sessions'):\n",
    "            batch = sessions[i:i+batch_size]\n",
    "            params = {'sids': batch}\n",
    "            cypher = \"\"\"UNWIND $sids AS sid\n",
    "            MERGE (s:Session {id: sid})\n",
    "            \"\"\"\n",
    "            session.run(cypher, **params)\n",
    "    print('Creating Event nodes and OCCURRED_IN relationships (this may take a while)')\n",
    "    with driver.session() as session:\n",
    "        for sid, group in tqdm(df.groupby('session_id'), desc='sessions_events', total=df['session_id'].nunique()):\n",
    "            g = group.sort_values('step_index')\n",
    "\n",
    "            start_ts = g['timestamp'].iloc[0].isoformat() if not pd.isnull(g['timestamp'].iloc[0]) else None\n",
    "            end_ts = g['timestamp'].iloc[-1].isoformat() if not pd.isnull(g['timestamp'].iloc[-1]) else None\n",
    "            session.run(\"\"\"MATCH (s:Session {id:$sid}) SET s.start_ts=$start_ts, s.end_ts=$end_ts, s.num_events=$num_events\"\"\", sid=sid, start_ts=start_ts, end_ts=end_ts, num_events=len(g))\n",
    "\n",
    "            for r in g.itertuples():\n",
    "                eid = str(r.event_id)\n",
    "                etype = getattr(r, 'event_type', None) if 'event_type' in g.columns else None\n",
    "                session.run(\"\"\"MERGE (e:Event {id:$eid})\n",
    "                ON CREATE SET e.type = $etype, e.global_count = 1\n",
    "                ON MATCH SET e.global_count = coalesce(e.global_count,0) + 1\n",
    "                \"\"\", eid=eid, etype=etype)\n",
    "                session.run(\"\"\"MATCH (e:Event {id:$eid}), (s:Session {id:$sid})\n",
    "                MERGE (e)-[r:OCCURRED_IN {index:$idx, ts:$ts}]->(s)\n",
    "                \"\"\", eid=eid, sid=sid, idx=int(r.step_index), ts=r.timestamp.isoformat() if not pd.isnull(r.timestamp) else None)\n",
    "    print('Event nodes and OCCURRED_IN relationships created.')\n",
    "\n",
    "# --- RE-DEFINE build_next_edges to include time ---\n",
    "\n",
    "def build_next_edges(df_train, driver, batch_size=2000):\n",
    "    \"\"\"\n",
    "    Calculates item-to-item transitions and creates NEXT relationships,\n",
    "    now including transition count and average time delta (duration).\n",
    "    \"\"\"\n",
    "    print(\"Building NEXT relationships with temporal properties...\")\n",
    "    \n",
    "    # 1. Aggregate transitions and time delta in Pandas\n",
    "    # NOTE: df_train must have 'step_index' and 'timestamp' columns.\n",
    "    \n",
    "    # Identify the next event in the session\n",
    "    df_transitions = df_train.copy()\n",
    "    df_transitions['next_itemid'] = df_transitions.groupby('session_id')['event_id'].shift(-1)\n",
    "    df_transitions['next_timestamp'] = df_transitions.groupby('session_id')['timestamp'].shift(-1)\n",
    "    \n",
    "    # Filter out the last event in each session (where next_itemid is NaN)\n",
    "    df_transitions.dropna(subset=['next_itemid'], inplace=True)\n",
    "    df_transitions['next_itemid'] = df_transitions['next_itemid'].astype(int)\n",
    "    \n",
    "    # Calculate the time delta in seconds (or milliseconds, depending on unit='ms' conversion)\n",
    "    df_transitions['time_delta_ms'] = (df_transitions['next_timestamp'] - df_transitions['timestamp']).dt.total_seconds() * 1000\n",
    "    \n",
    "    # Group and aggregate\n",
    "    transition_summary = df_transitions.groupby(['event_id', 'next_itemid']).agg(\n",
    "        count=('session_id', 'size'),\n",
    "        avg_duration_ms=('time_delta_ms', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # 2. Write transitions to Neo4j\n",
    "    \n",
    "    cypher_query = \"\"\"\n",
    "    UNWIND $batch AS row\n",
    "    MATCH (e1:Event {id: toString(row.event_id)})\n",
    "    MATCH (e2:Event {id: toString(row.next_itemid)})\n",
    "    MERGE (e1)-[r:NEXT]->(e2)\n",
    "    ON CREATE SET r.count = row.count, r.avg_duration_ms = row.avg_duration_ms\n",
    "    ON MATCH SET r.count = r.count + row.count, r.avg_duration_ms = row.avg_duration_ms\n",
    "    \"\"\"\n",
    "    \n",
    "    transitions = transition_summary.to_dict('records')\n",
    "    \n",
    "    print(f\"Total unique transitions to process: {len(transitions)}\")\n",
    "    with driver.session() as session:\n",
    "        for i in tqdm(range(0, len(transitions), batch_size), desc=\"Writing NEXT edges\"):\n",
    "            batch = transitions[i:i + batch_size]\n",
    "            session.execute_write(lambda tx: tx.run(cypher_query, batch=batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "094f2120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints ensured.\n",
      "Building nodes for 193183 sessions (batched)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sessions: 100%|██████████| 39/39 [00:01<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Event nodes and OCCURRED_IN relationships (this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sessions_events: 100%|██████████| 193183/193183 [1:11:22<00:00, 45.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event nodes and OCCURRED_IN relationships created.\n",
      "Building NEXT relationships with temporal properties...\n",
      "Total unique transitions to process: 416698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing NEXT edges: 100%|██████████| 209/209 [00:27<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph build functions ready. To run: instantiate driver and call build_graph_from_df_train(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "driver = get_driver()\n",
    "build_graph_from_df(df_train, driver)\n",
    "build_next_edges(df_train, driver)\n",
    "driver.close()\n",
    "print('Graph build functions ready. To run: instantiate driver and call build_graph_from_df_train(...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62e96827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('92233', 4)]\n"
     ]
    }
   ],
   "source": [
    "def recommend_next_transition(driver, last_event_id, topk=10):\n",
    "    q = \"\"\"MATCH (e:Event {id:$last})-[r:NEXT]->(cand:Event)\n",
    "    RETURN cand.id AS id, r.count AS score\n",
    "    ORDER BY r.count DESC\n",
    "    LIMIT $k\n",
    "    \"\"\"\n",
    "    with driver.session() as s:\n",
    "        res = s.run(q, last=last_event_id, k=topk)\n",
    "        return [(r['id'], r['score']) for r in res]\n",
    "\n",
    "driver = get_driver()\n",
    "print(recommend_next_transition(driver, \"12345\", topk=5))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f023630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TEMPORAL TRANSITION MODEL (RECENCY DECAY) \n",
    "\n",
    "def recommend_temporal_transition(driver, last_event_id, topk=10):\n",
    "    \"\"\"\n",
    "    Recommends based on a Temporal Transition Score, penalizing long average durations.\n",
    "    Score: r.count / log(r.avg_duration_ms + 1)\n",
    "    \n",
    "    The log-based function prioritizes transitions that happened quickly (low avg_duration_ms).\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (e:Event {id:$last})-[r:NEXT]->(cand:Event)\n",
    "    WITH cand.id AS id, r.count AS count, r.avg_duration_ms AS duration\n",
    "    \n",
    "    \n",
    "    WITH id, \n",
    "         count, \n",
    "         duration,\n",
    "         CASE \n",
    "             WHEN duration IS NULL OR duration <= 0 THEN count // Fallback for 0 or null duration\n",
    "             ELSE count / log(duration + 1)\n",
    "         END AS temporal_score\n",
    "         \n",
    "    RETURN id, temporal_score AS score\n",
    "    ORDER BY score DESC\n",
    "    LIMIT $k\n",
    "    \"\"\"\n",
    "    with driver.session() as s:\n",
    "        res = s.run(query, last=str(last_event_id), k=topk)\n",
    "        return [(r['id'], r['score']) for r in res]\n",
    "\n",
    "driver = get_driver()\n",
    "# Example with the Temporal Model\n",
    "last_event = '461686' \n",
    "print(f\"Temporal Model Recs for {last_event}:\")\n",
    "print(recommend_temporal_transition(driver, last_event, topk=5))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc401de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B -> A Pattern (Session ID, Start Step Index):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def find_a_b_a_patterns(driver, event_A_id, event_B_id, limit=5):\n",
    "    \"\"\"\n",
    "    Finds sessions where event A is followed by event B, which is then \n",
    "    followed by event A again (A -> B -> A pattern). Uses the OCCURRED_IN\n",
    "    relationship's index property to ensure order.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "    MATCH (a1:Event {id: $event_A_id})<-[r_a1:OCCURRED_IN]-(s:Session)\n",
    "    MATCH (s)<-[r_b:OCCURRED_IN]-(b:Event {id: $event_B_id})\n",
    "    MATCH (s)<-[r_a2:OCCURRED_IN]-(a2:Event {id: $event_A_id})\n",
    "    \n",
    "    // Ensure the sequence is A -> B -> A based on step_index\n",
    "    WHERE r_a1.index < r_b.index AND r_b.index < r_a2.index\n",
    "    \n",
    "    // Aggregate by session and find the first instance of this pattern\n",
    "    WITH s, min(r_a1.index) AS start_index\n",
    "    \n",
    "    RETURN s.id AS session_id, start_index\n",
    "    ORDER BY start_index DESC\n",
    "    LIMIT $limit\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, event_A_id=str(event_A_id), \n",
    "                                     event_B_id=str(event_B_id), limit=limit)\n",
    "        return [(r['session_id'], r['start_index']) for r in result]\n",
    "\n",
    "driver = get_driver()\n",
    "# Example: Using the two most popular events found in Cell 16: '461686' and '257040'\n",
    "print(\"A -> B -> A Pattern (Session ID, Start Step Index):\")\n",
    "print(find_a_b_a_patterns(driver, '461686', '257040', limit=3))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fab66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a94fcf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MRR evaluation on TEST data for Simple Transition Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [01:16<00:00, 144.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Simple Transition, Top 50) on TEST data: 0.4861419919068567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank(true_id, ranked_list):\n",
    "    try:\n",
    "        pos = ranked_list.index(true_id)\n",
    "        return 1.0 / (pos + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_mrr(df_eval, driver, recommend_func, topk=50):\n",
    "    \"\"\"\n",
    "    Evaluates MRR for a given recommendation function on a test DataFrame.\n",
    "    The function is updated to pass the session history for multi-step models.\n",
    "    \"\"\"\n",
    "    rr_scores = []\n",
    "    \n",
    "    for sid, group in tqdm(df_eval.groupby('session_id'), desc='Evaluating sessions', total=df_eval['session_id'].nunique()):\n",
    "        g = group.sort_values('step_index')\n",
    "        events = list(g['event_id'].astype(str))\n",
    "        \n",
    "        # Iterate over all transitions (event 'a' followed by event 'b')\n",
    "        for i, b in enumerate(events[1:]):\n",
    "            \n",
    "            # The current event 'a' is at index i, history is events up to and including 'a'\n",
    "            current_history = events[:i+1] \n",
    "            last_event_id = current_history[-1]\n",
    "            \n",
    "            if recommend_func.__name__ == 'recommend_next_transition':\n",
    "                recs = recommend_func(driver, last_event_id, topk=topk)\n",
    "            elif recommend_func.__name__ == 'recommend_next_popularity':\n",
    "                # Popularity ignores the history\n",
    "                recs = recommend_func(driver, None, topk=topk)\n",
    "            else:\n",
    "                 # Pass the *current history* for advanced models like multi-hop\n",
    "                recs = recommend_func(driver, current_history, topk=topk) \n",
    "                \n",
    "            ranked_ids = [r[0] for r in recs]\n",
    "            rr_scores.append(reciprocal_rank(str(b), ranked_ids))\n",
    "            \n",
    "    return np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "# Re-evaluate the Transition Model with the updated function signature\n",
    "driver = get_driver()\n",
    "print('Starting MRR evaluation on TEST data for Simple Transition Model...')\n",
    "mrr_transition = evaluate_mrr(df_test, driver, recommend_next_transition, topk=50)\n",
    "print('MRR (Simple Transition, Top 50) on TEST data:', mrr_transition)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600f795",
   "metadata": {},
   "source": [
    "### implemeting and evaluating comparison methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "583aba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity Baseline Example: [('461686', 4975), ('257040', 2917), ('309778', 2741), ('219512', 2609), ('320130', 2352)]\n"
     ]
    }
   ],
   "source": [
    "def recommend_next_popularity(driver, last_event_id, topk=50):\n",
    "    \"\"\"\n",
    "    Recommends the top K most globally frequent events.\n",
    "    This serves as a non-personalized baseline.\n",
    "    The 'last_event_id' argument is ignored for this baseline.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (e:Event)\n",
    "    RETURN e.id AS event_id, e.global_count AS score\n",
    "    ORDER BY score DESC\n",
    "    LIMIT $topk\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, topk=topk)\n",
    "        return [(r['event_id'], r['score']) for r in result]\n",
    "\n",
    "driver = get_driver()\n",
    "top_items = recommend_next_popularity(driver, '12345', topk=5)\n",
    "print(\"Popularity Baseline Example:\", top_items)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14ba5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Hop Co-Occurrence Example for history ['461686', '257040', '309778']:\n",
      "[('218794', 437), ('171878', 311), ('32581', 149), ('10572', 129), ('360487', 128)]\n"
     ]
    }
   ],
   "source": [
    "def recommend_multi_hop_co_occurrence(driver, event_history_ids, topk=10, max_depth=3):\n",
    "    \"\"\"\n",
    "    Recommends events based on their overall transition popularity \n",
    "    from *any* event in the recent history (up to max_depth).\n",
    "    This leverages the global graph structure for contextual insight.\n",
    "    \"\"\"\n",
    "    if not event_history_ids:\n",
    "        return recommend_next_popularity(driver, None, topk=topk) # Fallback to popularity\n",
    "        \n",
    "    # Take only the N most recent events from the history\n",
    "    recent_history = event_history_ids[-max_depth:] \n",
    "    \n",
    "    # Query: Find candidates connected via :NEXT to *any* event in the recent history\n",
    "    query = \"\"\"\n",
    "    UNWIND $recent_ids AS history_event_id\n",
    "    MATCH (e_hist:Event {id: history_event_id})-[r:NEXT]->(candidate:Event)\n",
    "    \n",
    "    // Exclude events already in the session history\n",
    "    WHERE NOT candidate.id IN $all_history_ids\n",
    "    \n",
    "    WITH candidate, sum(r.count) AS aggregated_score\n",
    "    RETURN candidate.id AS id, aggregated_score AS score\n",
    "    ORDER BY score DESC\n",
    "    LIMIT $topk\n",
    "    \"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # Pass the entire history to the query for exclusion purposes\n",
    "        result = session.run(query, recent_ids=[str(e) for e in recent_history], \n",
    "                                     all_history_ids=[str(e) for e in event_history_ids],\n",
    "                                     topk=topk)\n",
    "        return [(r['id'], r['score']) for r in result]\n",
    "\n",
    "driver = get_driver()\n",
    "# Example: Simulate a history of 3 events (e.g., [123, 456, 789])\n",
    "test_history = ['461686', '257040', '309778'] \n",
    "print(f\"Multi-Hop Co-Occurrence Example for history {test_history}:\")\n",
    "print(recommend_multi_hop_co_occurrence(driver, test_history, topk=5))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada53fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Evaluating Popularity Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [13:21<00:00, 13.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Popularity Baseline, Top 50): 0.00359\n",
      "\n",
      "2. Evaluating Simple Transition Model (1-Hop)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [01:00<00:00, 182.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Simple Transition, Top 50): 0.48614\n",
      "\n",
      "3. Evaluating Multi-Hop Co-Occurrence Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [01:15<00:00, 147.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Multi-Hop Co-Occurrence, Top 50): 0.15862\n",
      "\n",
      "--- Final MRR Comparison (on TEST Data) ---\n",
      "| Model | MRR@50 |\n",
      "| :--- | :--- |\n",
      "| Popularity Baseline | 0.00359 |\n",
      "| Simple Transition | 0.48614 |\n",
      "| Multi-Hop Co-Occurrence | 0.15862 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Evaluation on df_test \n",
    "driver = get_driver()\n",
    "topk_limit = 50\n",
    "\n",
    "# 1. Evaluating Popularity Baseline\n",
    "print(\"1. Evaluating Popularity Baseline...\")\n",
    "mrr_popularity = evaluate_mrr(df_test, driver, recommend_next_popularity, topk=topk_limit)\n",
    "print(f\"MRR (Popularity Baseline, Top {topk_limit}): {mrr_popularity:.5f}\")\n",
    "\n",
    "\n",
    "# 2. Simple Transition Model \n",
    "print(\"\\n2. Evaluating Simple Transition Model (1-Hop)...\")\n",
    "mrr_transition = evaluate_mrr(df_test, driver, recommend_next_transition, topk=topk_limit)\n",
    "print(f\"MRR (Simple Transition, Top {topk_limit}): {mrr_transition:.5f}\")\n",
    "\n",
    "\n",
    "# 3. Multi-Hop Co-Occurrence Model \n",
    "print(\"\\n3. Evaluating Multi-Hop Co-Occurrence Model...\")\n",
    "mrr_multi_hop = evaluate_mrr(df_test, driver, recommend_multi_hop_co_occurrence, topk=topk_limit)\n",
    "print(f\"MRR (Multi-Hop Co-Occurrence, Top {topk_limit}): {mrr_multi_hop:.5f}\")\n",
    "\n",
    "\n",
    "# 4. New: Temporal Transition Model (Recency Decay)\n",
    "print(\"\\n4. New: Evaluating Temporal Transition Model (Recency Decay)...\")\n",
    "mrr_temporal = evaluate_mrr(df_test, driver, recommend_temporal_transition, topk=topk_limit)\n",
    "print(f\"MRR (Temporal Transition, Top {topk_limit}): {mrr_temporal:.5f}\")\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Summary Report\n",
    "print(\"\\n--- Final MRR Comparison (on TEST Data) ---\")\n",
    "print(f\"| Model | MRR@{topk_limit} |\")\n",
    "print(\"| :--- | :--- |\")\n",
    "print(f\"| Popularity Baseline | {mrr_popularity:.5f} |\")\n",
    "print(f\"| Simple Transition | {mrr_transition:.5f} |\")\n",
    "print(f\"| Multi-Hop Co-Occurrence | {mrr_multi_hop:.5f} |\")\n",
    "print(f\"| **Temporal Transition (Recency Decay)** | **{mrr_temporal:.5f}** |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073faa2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbd5277f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
