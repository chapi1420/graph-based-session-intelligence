{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea5a04a",
   "metadata": {},
   "source": [
    "\n",
    "## graph based session intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bb844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nahomnadew/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/nahomnadew/.cache/kagglehub/datasets/retailrocket/ecommerce-dataset/versions/2\n"
     ]
    }
   ],
   "source": [
    "from genericpath import exists\n",
    "import kagglehub\n",
    "\n",
    "if not exists(\"data/.kaggle/events.csv\"):\n",
    "    path = kagglehub.dataset_download(\"retailrocket/ecommerce-dataset\")\n",
    "else:\n",
    "    path = \"data/.kaggle/events.csv\"\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8665a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial parameters\n",
    "\n",
    "DATA_PATH = \"data/kaggle/events.csv\"  \n",
    "SAMPLE_MAX_SESSIONS = 200000  \n",
    "MIN_SESSION_LENGTH = 2  \n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_AUTH = (\"neo4j\", \"nahi1420\")  \n",
    "BATCH_SIZE = 5000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be81e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 2.3.3\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"pandas\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1fcf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, this may take a while for the full RetailRocket dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146646/2477069091.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(DATA_PATH, parse_dates=['timestamp'], low_memory=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 2756101\n",
      "Columns: ['timestamp', 'visitorid', 'event', 'itemid', 'transactionid']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433221332117</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433224214164</td>\n",
       "      <td>992329</td>\n",
       "      <td>view</td>\n",
       "      <td>248676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433221999827</td>\n",
       "      <td>111016</td>\n",
       "      <td>view</td>\n",
       "      <td>318965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433221955914</td>\n",
       "      <td>483717</td>\n",
       "      <td>view</td>\n",
       "      <td>253185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433221337106</td>\n",
       "      <td>951259</td>\n",
       "      <td>view</td>\n",
       "      <td>367447</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp  visitorid event  itemid  transactionid\n",
       "0  1433221332117     257597  view  355908            NaN\n",
       "1  1433224214164     992329  view  248676            NaN\n",
       "2  1433221999827     111016  view  318965            NaN\n",
       "3  1433221955914     483717  view  253185            NaN\n",
       "4  1433221337106     951259  view  367447            NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset \n",
    "assert os.path.exists(DATA_PATH), f\"Data file not found: {DATA_PATH}\"\n",
    "print(\"Loading, this may take a while for the full RetailRocket dataset...\")\n",
    "\n",
    "# The RetailRocket 'events.csv' has columns similar to: sessionId, itemId, eventType, timestamp\n",
    "usecols = None  \n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed columns mapping: {'visitorid': 'session_id', 'itemid': 'event_id', 'event': 'event_type', 'timestamp': 'timestamp'}\n",
      "After filtering short sessions: 406020 unique sessions, 1754541 rows\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initial Column Renaming and Setup ---\n",
    "\n",
    "\n",
    "cols = [c.lower() for c in df.columns]\n",
    "colmap = {}\n",
    "\n",
    "# Map visitorid to user_id, NOT session_id\n",
    "if 'visitorid' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='visitorid'][0]] = 'user_id' \n",
    "if 'itemid' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='itemid'][0]] = 'event_id'\n",
    "if 'event' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='event'][0]] = 'event_type'\n",
    "if 'timestamp' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='timestamp'][0]] = 'timestamp'\n",
    "\n",
    "df = df.rename(columns=colmap)\n",
    "print('Renamed columns mapping:', colmap)\n",
    "assert 'user_id' in df.columns and 'event_id' in df.columns and 'timestamp' in df.columns\n",
    "\n",
    "# Convert timestamp to datetime and sort by user and time\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "df = df.sort_values(['user_id','timestamp'])\n",
    "\n",
    "\n",
    "# --- 2. The CRITICAL Sessionization Step ---\n",
    "\n",
    "df['time_diff'] = df.groupby('user_id')['timestamp'].diff()\n",
    "\n",
    "MAX_SESSION_GAP = pd.Timedelta('30 minutes')\n",
    "\n",
    "\n",
    "df['new_session'] = (df['time_diff'].isnull()) | (df['time_diff'] > MAX_SESSION_GAP)\n",
    "\n",
    "# Create the session_id by cumulatively summing the 'new_session' flag and grouping by user\n",
    "df['session_id'] = df.groupby('user_id')['new_session'].cumsum().astype(str) + '_' + df['user_id'].astype(str)\n",
    "\n",
    "# Add step_index per session (order by timestamp)\n",
    "df['step_index'] = df.groupby('session_id').cumcount() + 1\n",
    "df = df.drop(columns=['time_diff', 'new_session']) # Cleanup intermediate columns\n",
    "\n",
    "\n",
    "\n",
    "# Filter out sessions that are too short (length < MIN_SESSION_LENGTH)\n",
    "session_lengths = df.groupby('session_id').size()\n",
    "valid_sessions = session_lengths[session_lengths >= MIN_SESSION_LENGTH].index\n",
    "df = df[df['session_id'].isin(valid_sessions)].copy()\n",
    "\n",
    "print(\"\\n--- Sessionization Complete ---\")\n",
    "print(f\"Total unique VISITOR IDs: {df['user_id'].nunique()}\")\n",
    "print(f\"Total unique SESSIONS (after 30-min gap rule): {df['session_id'].nunique()}\")\n",
    "print(f\"After filtering short sessions (<{MIN_SESSION_LENGTH}): {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c608219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled sessions: 200000 rows: 869349\n"
     ]
    }
   ],
   "source": [
    "# pick the first N sessions (keeps temporal order inside sessions)\n",
    "if SAMPLE_MAX_SESSIONS is not None:\n",
    "    unique_sess = df['session_id'].drop_duplicates().iloc[:SAMPLE_MAX_SESSIONS]\n",
    "    df = df[df['session_id'].isin(unique_sess)].copy()\n",
    "    print(\"Sampled sessions:\", df['session_id'].nunique(), \"rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe88d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample to data/sampled_sessions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save a sampled CSV for quick checks \n",
    "sample_out = \"data/sampled_sessions.csv\"\n",
    "df.to_csv(sample_out, index=False)\n",
    "print(\"Saved sample to\", sample_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96663298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146646/3748043458.py:1: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n"
     ]
    }
   ],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97860eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Range Check ---\n",
      "First event timestamp: 2015-05-03 03:00:04.384000\n",
      "Last event timestamp: 2015-09-18 02:57:40.810000\n"
     ]
    }
   ],
   "source": [
    "#Check Data Range and Determine Cutoff \n",
    "print(\"\\n--- Data Range Check ---\")\n",
    "min_ts = df['timestamp'].min()\n",
    "max_ts = df['timestamp'].max()\n",
    "print(f\"First event timestamp: {min_ts}\")\n",
    "print(f\"Last event timestamp: {max_ts}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52cd1fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Cutoff Date: 2015-09-11 02:57:40.810000\n",
      "Train sessions: 193183 (Rows: 834594)\n",
      "Test sessions: 11074 (Rows: 34755)\n"
     ]
    }
   ],
   "source": [
    "total_duration = df['timestamp'].max() - df['timestamp'].min()\n",
    "SEVEN_DAYS_DELTA = pd.Timedelta(days=7)\n",
    "SPLIT_CUTOFF_DATE = df['timestamp'].max() - SEVEN_DAYS_DELTA\n",
    "\n",
    "df_train = df[df['timestamp'] < SPLIT_CUTOFF_DATE].copy()\n",
    "df_test = df[df['timestamp'] >= SPLIT_CUTOFF_DATE].copy()\n",
    "\n",
    "\n",
    "print(f\"Split Cutoff Date: {SPLIT_CUTOFF_DATE}\")\n",
    "print(f\"Train sessions: {df_train['session_id'].nunique()} (Rows: {len(df_train)})\")\n",
    "print(f\"Test sessions: {df_test['session_id'].nunique()} (Rows: {len(df_test)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd87aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j connection helper\n",
    "def get_driver(uri=NEO4J_URI, auth=NEO4J_AUTH):\n",
    "    return GraphDatabase.driver(uri, auth=auth, encrypted=False)\n",
    "\n",
    "def create_constraints(driver):\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(lambda tx: tx.run(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Event) REQUIRE e.id IS UNIQUE\"\"\"))\n",
    "        session.execute_write(lambda tx: tx.run(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (s:Session) REQUIRE s.id IS UNIQUE\"\"\"))\n",
    "    print('Constraints ensured.')\n",
    "\n",
    "def build_graph_from_df(df, driver, batch_size=BATCH_SIZE):\n",
    "    # df must contain: session_id, timestamp, step_index, event_id, event_type (optional)\n",
    "    create_constraints(driver)\n",
    "    sessions = df['session_id'].drop_duplicates().tolist()\n",
    "    print('Building nodes for', len(sessions), 'sessions (batched)')\n",
    "    with driver.session() as session:\n",
    "        for i in tqdm(range(0, len(sessions), batch_size), desc='sessions'):\n",
    "            batch = sessions[i:i+batch_size]\n",
    "            params = {'sids': batch}\n",
    "            cypher = \"\"\"UNWIND $sids AS sid\n",
    "            MERGE (s:Session {id: sid})\n",
    "            \"\"\"\n",
    "            session.run(cypher, **params)\n",
    "    print('Creating Event nodes and OCCURRED_IN relationships (this may take a while)')\n",
    "    with driver.session() as session:\n",
    "        for sid, group in tqdm(df.groupby('session_id'), desc='sessions_events', total=df['session_id'].nunique()):\n",
    "            g = group.sort_values('step_index')\n",
    "\n",
    "            start_ts = g['timestamp'].iloc[0].isoformat() if not pd.isnull(g['timestamp'].iloc[0]) else None\n",
    "            end_ts = g['timestamp'].iloc[-1].isoformat() if not pd.isnull(g['timestamp'].iloc[-1]) else None\n",
    "            session.run(\"\"\"MATCH (s:Session {id:$sid}) SET s.start_ts=$start_ts, s.end_ts=$end_ts, s.num_events=$num_events\"\"\", sid=sid, start_ts=start_ts, end_ts=end_ts, num_events=len(g))\n",
    "\n",
    "            for r in g.itertuples():\n",
    "                eid = str(r.event_id)\n",
    "                etype = getattr(r, 'event_type', None) if 'event_type' in g.columns else None\n",
    "                session.run(\"\"\"MERGE (e:Event {id:$eid})\n",
    "                ON CREATE SET e.type = $etype, e.global_count = 1\n",
    "                ON MATCH SET e.global_count = coalesce(e.global_count,0) + 1\n",
    "                \"\"\", eid=eid, etype=etype)\n",
    "                session.run(\"\"\"MATCH (e:Event {id:$eid}), (s:Session {id:$sid})\n",
    "                MERGE (e)-[r:OCCURRED_IN {index:$idx, ts:$ts}]->(s)\n",
    "                \"\"\", eid=eid, sid=sid, idx=int(r.step_index), ts=r.timestamp.isoformat() if not pd.isnull(r.timestamp) else None)\n",
    "    print('Event nodes and OCCURRED_IN relationships created.')\n",
    "\n",
    "# --- RE-DEFINE build_next_edges to include time ---\n",
    "\n",
    "def build_next_edges(df_train, driver, batch_size=2000):\n",
    "    \"\"\"\n",
    "    Calculates item-to-item transitions and creates NEXT relationships,\n",
    "    now including transition count and average time delta (duration).\n",
    "    \"\"\"\n",
    "    print(\"Building NEXT relationships with temporal properties...\")\n",
    "    \n",
    "    # 1. Aggregate transitions and time delta in Pandas\n",
    "    \n",
    "    # Identify the next event in the session\n",
    "    df_transitions = df_train.copy()\n",
    "    df_transitions['next_itemid'] = df_transitions.groupby('session_id')['event_id'].shift(-1)\n",
    "    df_transitions['next_timestamp'] = df_transitions.groupby('session_id')['timestamp'].shift(-1)\n",
    "    \n",
    "    # Filter out the last event in each session (where next_itemid is NaN)\n",
    "    df_transitions.dropna(subset=['next_itemid'], inplace=True)\n",
    "    df_transitions['next_itemid'] = df_transitions['next_itemid'].astype(int)\n",
    "    \n",
    "    # Calculate the time delta in seconds (or milliseconds, depending on unit='ms' conversion)\n",
    "    df_transitions['time_delta_ms'] = (df_transitions['next_timestamp'] - df_transitions['timestamp']).dt.total_seconds() * 1000\n",
    "    \n",
    "    # Group and aggregate\n",
    "    transition_summary = df_transitions.groupby(['event_id', 'next_itemid']).agg(\n",
    "        count=('session_id', 'size'),\n",
    "        avg_duration_ms=('time_delta_ms', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # 2. Write transitions to Neo4j\n",
    "    \n",
    "    cypher_query = \"\"\"\n",
    "    UNWIND $batch AS row\n",
    "    MATCH (e1:Event {id: toString(row.event_id)})\n",
    "    MATCH (e2:Event {id: toString(row.next_itemid)})\n",
    "    MERGE (e1)-[r:NEXT]->(e2)\n",
    "    ON CREATE SET r.count = row.count, r.avg_duration_ms = row.avg_duration_ms\n",
    "    ON MATCH SET r.count = r.count + row.count, r.avg_duration_ms = row.avg_duration_ms\n",
    "    \"\"\"\n",
    "    \n",
    "    transitions = transition_summary.to_dict('records')\n",
    "    \n",
    "    print(f\"Total unique transitions to process: {len(transitions)}\")\n",
    "    with driver.session() as session:\n",
    "        for i in tqdm(range(0, len(transitions), batch_size), desc=\"Writing NEXT edges\"):\n",
    "            batch = transitions[i:i + batch_size]\n",
    "            session.execute_write(lambda tx: tx.run(cypher_query, batch=batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "094f2120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints ensured.\n",
      "Building nodes for 193183 sessions (batched)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sessions: 100%|██████████| 39/39 [00:00<00:00, 62.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Event nodes and OCCURRED_IN relationships (this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sessions_events:  41%|████▏     | 79876/193183 [10:56<15:30, 121.71it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m driver = get_driver()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbuild_graph_from_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m build_next_edges(df_train, driver)\n\u001b[32m      4\u001b[39m driver.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mbuild_graph_from_df\u001b[39m\u001b[34m(df, driver, batch_size)\u001b[39m\n\u001b[32m     34\u001b[39m             eid = \u001b[38;5;28mstr\u001b[39m(r.event_id)\n\u001b[32m     35\u001b[39m             etype = \u001b[38;5;28mgetattr\u001b[39m(r, \u001b[33m'\u001b[39m\u001b[33mevent_type\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mevent_type\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m g.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m             \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[33;43mMERGE (e:Event \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mid:$eid})\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[33;43m            ON CREATE SET e.type = $etype, e.global_count = 1\u001b[39;49m\n\u001b[32m     38\u001b[39m \u001b[33;43m            ON MATCH SET e.global_count = coalesce(e.global_count,0) + 1\u001b[39;49m\n\u001b[32m     39\u001b[39m \u001b[33;43m            \u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meid\u001b[49m\u001b[43m=\u001b[49m\u001b[43meid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m=\u001b[49m\u001b[43metype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m             session.run(\u001b[33m\"\"\"\u001b[39m\u001b[33mMATCH (e:Event \u001b[39m\u001b[33m{\u001b[39m\u001b[33mid:$eid}), (s:Session \u001b[39m\u001b[33m{\u001b[39m\u001b[33mid:$sid})\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[33m            MERGE (e)-[r:OCCURRED_IN \u001b[39m\u001b[33m{\u001b[39m\u001b[33mindex:$idx, ts:$ts}]->(s)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[33m            \u001b[39m\u001b[33m\"\"\"\u001b[39m, eid=eid, sid=sid, idx=\u001b[38;5;28mint\u001b[39m(r.step_index), ts=r.timestamp.isoformat() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd.isnull(r.timestamp) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEvent nodes and OCCURRED_IN relationships created.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_sync/work/session.py:320\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, parameters, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m bookmarks = \u001b[38;5;28mself\u001b[39m._get_bookmarks()\n\u001b[32m    319\u001b[39m parameters = \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotifications_disabled_classifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._auto_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_sync/work/result.py:235\u001b[39m, in \u001b[36mResult._run\u001b[39m\u001b[34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[39m\n\u001b[32m    218\u001b[39m     Util.callback(\u001b[38;5;28mself\u001b[39m._on_closed)\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m._connection.run(\n\u001b[32m    221\u001b[39m     query_text,\n\u001b[32m    222\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m     on_failure=on_failed_attach,\n\u001b[32m    234\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m._connection.send_all()\n\u001b[32m    237\u001b[39m \u001b[38;5;28mself\u001b[39m._attach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_sync/work/result.py:274\u001b[39m, in \u001b[36mResult._pull\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    271\u001b[39m     _on_summary()\n\u001b[32m    272\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_warnings()\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpull\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_qid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hydration_scope\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_records\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_records\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_success\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_success\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_failure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_failure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;28mself\u001b[39m._streaming = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:192\u001b[39m, in \u001b[36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwargs):\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio.iscoroutinefunction(\u001b[38;5;28mself\u001b[39m.__on_error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:1100\u001b[39m, in \u001b[36mBolt5x5.pull\u001b[39m\u001b[34m(self, n, qid, dehydration_hooks, hydration_hooks, **handlers)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpull\u001b[39m(\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1091\u001b[39m     n=-\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1095\u001b[39m     **handlers,\n\u001b[32m   1096\u001b[39m ):\n\u001b[32m   1097\u001b[39m     handlers[\u001b[33m\"\u001b[39m\u001b[33mon_success\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._make_enrich_statuses_handler(\n\u001b[32m   1098\u001b[39m         wrapped_handler=handlers.get(\u001b[33m\"\u001b[39m\u001b[33mon_success\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1099\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdehydration_hooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhandlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:332\u001b[39m, in \u001b[36mBolt5x0.pull\u001b[39m\u001b[34m(self, n, qid, dehydration_hooks, hydration_hooks, **handlers)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpull\u001b[39m(\n\u001b[32m    325\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    326\u001b[39m     n=-\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m     **handlers,\n\u001b[32m    331\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     dehydration_hooks, hydration_hooks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_hydration_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdehydration_hooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     extra = {\u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n}\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m qid != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:794\u001b[39m, in \u001b[36mBolt._default_hydration_hooks\u001b[39m\u001b[34m(self, dehydration_hooks, hydration_hooks)\u001b[39m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dehydration_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hydration_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dehydration_hooks, hydration_hooks\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m hydration_scope = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_hydration_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dehydration_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    796\u001b[39m     dehydration_hooks = hydration_scope.dehydration_hooks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:789\u001b[39m, in \u001b[36mBolt.new_hydration_scope\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_hydration_scope\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhydration_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_hydration_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/neo4j/_codec/hydration/v2/hydration_handler.py:101\u001b[39m, in \u001b[36mHydrationHandler.new_hydration_scope\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_hydration_scope\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._created_scope = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHydrationScope\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_GraphHydrator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "driver = get_driver()\n",
    "build_graph_from_df(df_train, driver)\n",
    "build_next_edges(df_train, driver)\n",
    "driver.close()\n",
    "print('Graph build functions ready. To run: instantiate driver and call build_graph_from_df_train(...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e96827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('92233', 5)]\n"
     ]
    }
   ],
   "source": [
    "def recommend_next_transition(driver, last_event_id, topk=10):\n",
    "    q = \"\"\"MATCH (e:Event {id:$last})-[r:NEXT]->(cand:Event)\n",
    "    RETURN cand.id AS id, r.count AS score\n",
    "    ORDER BY r.count DESC\n",
    "    LIMIT $k\n",
    "    \"\"\"\n",
    "    with driver.session() as s:\n",
    "        res = s.run(q, last=last_event_id, k=topk)\n",
    "        return [(r['id'], r['score']) for r in res]\n",
    "\n",
    "driver = get_driver()\n",
    "print(recommend_next_transition(driver, \"12345\", topk=5))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f023630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Model Recs for 461686:\n",
      "[('461686', 139.0786297097485), ('218794', 30.529432322018035), ('171878', 24.171619993465768), ('32581', 11.11591825243792), ('10572', 9.623820130996977)]\n"
     ]
    }
   ],
   "source": [
    "#  TEMPORAL TRANSITION MODEL (RECENCY DECAY) \n",
    "\n",
    "def recommend_temporal_transition(driver, last_event_id, topk=10):\n",
    "    \"\"\"\n",
    "    Recommends based on a Temporal Transition Score, penalizing long average durations.\n",
    "    Score: r.count / log(r.avg_duration_ms + 1)\n",
    "    \n",
    "    The log-based function prioritizes transitions that happened quickly (low avg_duration_ms).\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (e:Event {id:$last})-[r:NEXT]->(cand:Event)\n",
    "    WITH cand.id AS id, r.count AS count, r.avg_duration_ms AS duration\n",
    "    \n",
    "    \n",
    "    WITH id, \n",
    "         count, \n",
    "         duration,\n",
    "         CASE \n",
    "             WHEN duration IS NULL OR duration <= 0 THEN count // Fallback for 0 or null duration\n",
    "             ELSE count / log(duration + 1)\n",
    "         END AS temporal_score\n",
    "         \n",
    "    RETURN id, temporal_score AS score\n",
    "    ORDER BY score DESC\n",
    "    LIMIT $k\n",
    "    \"\"\"\n",
    "    with driver.session() as s:\n",
    "        res = s.run(query, last=str(last_event_id), k=topk)\n",
    "        return [(r['id'], r['score']) for r in res]\n",
    "\n",
    "driver = get_driver()\n",
    "# Example with the Temporal Model\n",
    "last_event = '461686' \n",
    "print(f\"Temporal Model Recs for {last_event}:\")\n",
    "print(recommend_temporal_transition(driver, last_event, topk=5))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHTED TEMPORAL TRANSITION MODEL (DIFFERENTIAL ACTION WEIGHTING) \n",
    "\n",
    "def recommend_weighted_temporal_transition(driver, last_event_id, topk=10):\n",
    "    \"\"\"\n",
    "    Recommends by combining Temporal Decay and Differential Action Weighting.\n",
    "    Weights: transaction=5.0, addtocart=3.0, view=1.0 (assumes e.type property exists)\n",
    "    Score: (r.count * action_weight) / log(r.avg_duration_ms + 1)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (e:Event {id:$last})-[r:NEXT]->(cand:Event)\n",
    "    \n",
    "    // 1. Define the action weight based on the type of the last event (e)\n",
    "    WITH cand.id AS id, r.count AS count, r.avg_duration_ms AS duration, e.type AS last_type,\n",
    "         CASE e.type\n",
    "             WHEN 'transaction' THEN 5.0\n",
    "             WHEN 'addtocart'   THEN 3.0\n",
    "             ELSE 1.0 // Default for 'view' or other types\n",
    "         END AS action_weight\n",
    "    \n",
    "    // 2. Calculate the weighted count\n",
    "    WITH id, duration, (count * action_weight) AS weighted_count, last_type\n",
    "    \n",
    "    // 3. Apply the Recency Decay (Temporal Score) logic\n",
    "    // The final score is the weighted frequency scaled by the recency decay (log(duration + 1))\n",
    "    WITH id, \n",
    "         weighted_count, \n",
    "         duration,\n",
    "         last_type,\n",
    "         CASE \n",
    "             WHEN duration IS NULL OR duration <= 0 THEN weighted_count // Fallback\n",
    "             ELSE weighted_count / log(duration + 1)\n",
    "         END AS temporal_score\n",
    "         \n",
    "    RETURN id, temporal_score AS score, last_type\n",
    "    ORDER BY score DESC\n",
    "    LIMIT $k\n",
    "    \"\"\"\n",
    "    with driver.session() as s:\n",
    "        res = s.run(query, last=str(last_event_id), k=topk)\n",
    "        return [(r['id'], r['score']) for r in res]\n",
    "\n",
    "driver = get_driver()\n",
    "# Example with the Weighted Temporal Model\n",
    "last_event = '461686' \n",
    "print(f\"Weighted Temporal Model Recs for {last_event}:\")\n",
    "print(recommend_weighted_temporal_transition(driver, last_event, topk=5))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc401de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B -> A Pattern (Session ID, Start Step Index):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def find_a_b_a_patterns(driver, event_A_id, event_B_id, limit=5):\n",
    "    \"\"\"\n",
    "    Finds sessions where event A is followed by event B, which is then \n",
    "    followed by event A again (A -> B -> A pattern). Uses the OCCURRED_IN\n",
    "    relationship's index property to ensure order.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "    MATCH (a1:Event {id: $event_A_id})<-[r_a1:OCCURRED_IN]-(s:Session)\n",
    "    MATCH (s)<-[r_b:OCCURRED_IN]-(b:Event {id: $event_B_id})\n",
    "    MATCH (s)<-[r_a2:OCCURRED_IN]-(a2:Event {id: $event_A_id})\n",
    "    \n",
    "    // Ensure the sequence is A -> B -> A based on step_index\n",
    "    WHERE r_a1.index < r_b.index AND r_b.index < r_a2.index\n",
    "    \n",
    "    // Aggregate by session and find the first instance of this pattern\n",
    "    WITH s, min(r_a1.index) AS start_index\n",
    "    \n",
    "    RETURN s.id AS session_id, start_index\n",
    "    ORDER BY start_index DESC\n",
    "    LIMIT $limit\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, event_A_id=str(event_A_id), \n",
    "                                     event_B_id=str(event_B_id), limit=limit)\n",
    "        return [(r['session_id'], r['start_index']) for r in result]\n",
    "\n",
    "driver = get_driver()\n",
    "# Example: Using the two most popular events found in Cell 16: '461686' and '257040'\n",
    "print(\"A -> B -> A Pattern (Session ID, Start Step Index):\")\n",
    "print(find_a_b_a_patterns(driver, '461686', '257040', limit=3))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fab66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fcf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MRR evaluation on TEST data for Simple Transition Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [00:55<00:00, 198.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Simple Transition, Top 50) on TEST data: 0.4837387297407908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank(true_id, ranked_list):\n",
    "    try:\n",
    "        pos = ranked_list.index(true_id)\n",
    "        return 1.0 / (pos + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_mrr(df_eval, driver, recommend_func, topk=50):\n",
    "    \"\"\"\n",
    "    Evaluates MRR for a given recommendation function on a test DataFrame.\n",
    "    The function is updated to pass the session history for multi-step models.\n",
    "    \"\"\"\n",
    "    rr_scores = []\n",
    "    \n",
    "    for sid, group in tqdm(df_eval.groupby('session_id'), desc='Evaluating sessions', total=df_eval['session_id'].nunique()):\n",
    "        g = group.sort_values('step_index')\n",
    "        events = list(g['event_id'].astype(str))\n",
    "        \n",
    "        # Iterate over all transitions (event 'a' followed by event 'b')\n",
    "        for i, b in enumerate(events[1:]):\n",
    "            \n",
    "            # The current event 'a' is at index i, history is events up to and including 'a'\n",
    "            current_history = events[:i+1] \n",
    "            last_event_id = current_history[-1]\n",
    "            \n",
    "            if recommend_func.__name__ == 'recommend_next_transition':\n",
    "                recs = recommend_func(driver, last_event_id, topk=topk)\n",
    "            elif recommend_func.__name__ == 'recommend_next_popularity':\n",
    "                # Popularity ignores the history\n",
    "                recs = recommend_func(driver, None, topk=topk)\n",
    "            else:\n",
    "                 # Pass the *current history* for advanced models like multi-hop\n",
    "                recs = recommend_func(driver, current_history, topk=topk) \n",
    "                \n",
    "            ranked_ids = [r[0] for r in recs]\n",
    "            rr_scores.append(reciprocal_rank(str(b), ranked_ids))\n",
    "            \n",
    "    return np.mean(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "# Re-evaluate the Transition Model with the updated function signature\n",
    "driver = get_driver()\n",
    "print('Starting MRR evaluation on TEST data for Simple Transition Model...')\n",
    "mrr_transition = evaluate_mrr(df_test, driver, recommend_next_transition, topk=50)\n",
    "print('MRR (Simple Transition, Top 50) on TEST data:', mrr_transition)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600f795",
   "metadata": {},
   "source": [
    "### implemeting and evaluating comparison methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583aba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity Baseline Example: [('461686', 6203), ('257040', 3646), ('309778', 3426), ('219512', 3253), ('320130', 2924)]\n"
     ]
    }
   ],
   "source": [
    "def recommend_next_popularity(driver, last_event_id, topk=50):\n",
    "    \"\"\"\n",
    "    Recommends the top K most globally frequent events.\n",
    "    This serves as a non-personalized baseline.\n",
    "    The 'last_event_id' argument is ignored for this baseline.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MATCH (e:Event)\n",
    "    RETURN e.id AS event_id, e.global_count AS score\n",
    "    ORDER BY score DESC\n",
    "    LIMIT $topk\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, topk=topk)\n",
    "        return [(r['event_id'], r['score']) for r in result]\n",
    "\n",
    "driver = get_driver()\n",
    "top_items = recommend_next_popularity(driver, '12345', topk=5)\n",
    "print(\"Popularity Baseline Example:\", top_items)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Hop Co-Occurrence Example for history ['461686', '257040', '309778']:\n",
      "[('218794', 545), ('171878', 387), ('32581', 186), ('10572', 160), ('360487', 160)]\n"
     ]
    }
   ],
   "source": [
    "def recommend_multi_hop_co_occurrence(driver, event_history_ids, topk=10, max_depth=3):\n",
    "    \"\"\"\n",
    "    Recommends events based on their overall transition popularity \n",
    "    from *any* event in the recent history (up to max_depth).\n",
    "    This leverages the global graph structure for contextual insight.\n",
    "    \"\"\"\n",
    "    if not event_history_ids:\n",
    "        return recommend_next_popularity(driver, None, topk=topk) # Fallback to popularity\n",
    "        \n",
    "    # Take only the N most recent events from the history\n",
    "    recent_history = event_history_ids[-max_depth:] \n",
    "    \n",
    "    # Query: Find candidates connected via :NEXT to *any* event in the recent history\n",
    "    query = \"\"\"\n",
    "    UNWIND $recent_ids AS history_event_id\n",
    "    MATCH (e_hist:Event {id: history_event_id})-[r:NEXT]->(candidate:Event)\n",
    "    \n",
    "    // Exclude events already in the session history\n",
    "    WHERE NOT candidate.id IN $all_history_ids\n",
    "    \n",
    "    WITH candidate, sum(r.count) AS aggregated_score\n",
    "    RETURN candidate.id AS id, aggregated_score AS score\n",
    "    ORDER BY score DESC\n",
    "    LIMIT $topk\n",
    "    \"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # Pass the entire history to the query for exclusion purposes\n",
    "        result = session.run(query, recent_ids=[str(e) for e in recent_history], \n",
    "                                     all_history_ids=[str(e) for e in event_history_ids],\n",
    "                                     topk=topk)\n",
    "        return [(r['id'], r['score']) for r in result]\n",
    "\n",
    "driver = get_driver()\n",
    "# Example: Simulate a history of 3 events (e.g., [123, 456, 789])\n",
    "test_history = ['461686', '257040', '309778'] \n",
    "print(f\"Multi-Hop Co-Occurrence Example for history {test_history}:\")\n",
    "print(recommend_multi_hop_co_occurrence(driver, test_history, topk=5))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada53fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Evaluating Popularity Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [11:05<00:00, 16.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Popularity Baseline, Top 50): 0.00359\n",
      "\n",
      "2. Evaluating Simple Transition Model (1-Hop)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [00:20<00:00, 535.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Simple Transition, Top 50): 0.48374\n",
      "\n",
      "3. Evaluating Multi-Hop Co-Occurrence Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [00:41<00:00, 269.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Multi-Hop Co-Occurrence, Top 50): 0.15834\n",
      "\n",
      "4. New: Evaluating Temporal Transition Model (Recency Decay)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sessions: 100%|██████████| 11074/11074 [00:14<00:00, 789.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Temporal Transition, Top 50): 0.00000\n",
      "\n",
      "--- Final MRR Comparison (on TEST Data) ---\n",
      "| Model | MRR@50 |\n",
      "| :--- | :--- |\n",
      "| Popularity Baseline | 0.00359 |\n",
      "| Simple Transition | 0.48374 |\n",
      "| Multi-Hop Co-Occurrence | 0.15834 |\n",
      "| **Temporal Transition (Recency Decay)** | **0.00000** |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Evaluation on df_test \n",
    "driver = get_driver()\n",
    "topk_limit = 50\n",
    "\n",
    "# 1. Evaluating Popularity Baseline\n",
    "print(\"1. Evaluating Popularity Baseline...\")\n",
    "mrr_popularity = evaluate_mrr(df_test, driver, recommend_next_popularity, topk=topk_limit)\n",
    "print(f\"MRR (Popularity Baseline, Top {topk_limit}): {mrr_popularity:.5f}\")\n",
    "\n",
    "\n",
    "# 2. Simple Transition Model \n",
    "print(\"\\n2. Evaluating Simple Transition Model (1-Hop)...\")\n",
    "mrr_transition = evaluate_mrr(df_test, driver, recommend_next_transition, topk=topk_limit)\n",
    "print(f\"MRR (Simple Transition, Top {topk_limit}): {mrr_transition:.5f}\")\n",
    "\n",
    "\n",
    "# 3. Multi-Hop Co-Occurrence Model \n",
    "print(\"\\n3. Evaluating Multi-Hop Co-Occurrence Model...\")\n",
    "mrr_multi_hop = evaluate_mrr(df_test, driver, recommend_multi_hop_co_occurrence, topk=topk_limit)\n",
    "print(f\"MRR (Multi-Hop Co-Occurrence, Top {topk_limit}): {mrr_multi_hop:.5f}\")\n",
    "\n",
    "\n",
    "# 4. Temporal Transition Model (Recency Decay) - Unweighted\n",
    "print(\"\\n4. Evaluating Temporal Transition Model (Recency Decay - Unweighted)...\")\n",
    "mrr_temporal = evaluate_mrr(df_test, driver, recommend_temporal_transition, topk=topk_limit)\n",
    "print(f\"MRR (Temporal Transition, Top {topk_limit}): {mrr_temporal:.5f}\")\n",
    "\n",
    "\n",
    "# 5. Weighted Temporal Transition Model\n",
    "print(\"\\n5. NEW: Evaluating Weighted Temporal Transition Model (Intent + Recency)...\")\n",
    "mrr_weighted_temporal = evaluate_mrr(df_test, driver, recommend_weighted_temporal_transition, topk=topk_limit)\n",
    "print(f\"MRR (Weighted Temporal Transition, Top {topk_limit}): {mrr_weighted_temporal:.5f}\")\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Summary Report\n",
    "print(\"\\n--- Final MRR Comparison (on TEST Data) ---\")\n",
    "print(f\"| Model | MRR@{topk_limit} |\")\n",
    "print(\"| :--- | :--- |\")\n",
    "print(f\"| Popularity Baseline | {mrr_popularity:.5f} |\")\n",
    "print(f\"| Simple Transition | {mrr_transition:.5f} |\")\n",
    "print(f\"| Multi-Hop Co-Occurrence | {mrr_multi_hop:.5f} |\")\n",
    "print(f\"| Temporal Transition (Recency Decay) | {mrr_temporal:.5f} |\")\n",
    "print(f\"| **Weighted Temporal (Intent + Recency)** | **{mrr_weighted_temporal:.5f}** |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073faa2",
   "metadata": {},
   "source": [
    "### dashboard implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bb9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotly, Pandas, and defaultdict are available.\n",
      "\n",
      "--- Extracting Top 10 Most Popular Events ---\n",
      "Extracted 10 top events.\n",
      "\n",
      "--- Extracting Average Transition Duration Distribution ---\n",
      "Extracted 4 duration bins.\n",
      "\n",
      "--- Extracting Event Type Popularity ---\n",
      "Extracted 3 event types.\n",
      "\n",
      "Generating Top 10 Events Bar Chart...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating Top 10 Events Bar Chart...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m fig_events = px.bar(\n\u001b[32m     73\u001b[39m     df_top_events, \n\u001b[32m     74\u001b[39m     x=\u001b[33m'\u001b[39m\u001b[33meventId\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     color_continuous_scale=px.colors.sequential.Viridis\n\u001b[32m     80\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mfig_events\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# 2. Average Transition Duration Distribution (Bar Chart)\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating Transition Duration Distribution...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/plotly/basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/iCog/graph-based-SI/graph-env/lib/python3.12/site-packages/plotly/io/_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP AND DATA EXTRACTION FOR DASHBOARD ---\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "print(\"Plotly, Pandas, and defaultdict are available.\")\n",
    "\n",
    "\n",
    "# --- Helper Function to Run Cypher Queries ---\n",
    "def run_cypher_query(driver, query):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        # Convert results to a list of dictionaries for easy plotting\n",
    "        return [r.data() for r in result]\n",
    "\n",
    "driver = get_driver()\n",
    "\n",
    "# --- Data Extraction Queries (Define the Variables) ---\n",
    "\n",
    "# 1. Top 10 Most Popular Events\n",
    "print(\"\\n--- Extracting Top 10 Most Popular Events ---\")\n",
    "query_top_events = \"\"\"\n",
    "MATCH (e:Event)\n",
    "RETURN e.id AS eventId, e.global_count AS count\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "top_events_data = run_cypher_query(driver, query_top_events)\n",
    "print(f\"Extracted {len(top_events_data)} top events.\")\n",
    "\n",
    "\n",
    "# 2. Average Duration of Transitions (Distribution)\n",
    "print(\"\\n--- Extracting Average Transition Duration Distribution ---\")\n",
    "query_avg_duration_distribution = \"\"\"\n",
    "MATCH ()-[r:NEXT]->()\n",
    "RETURN \n",
    "    CASE \n",
    "        WHEN r.avg_duration_ms < 1000 THEN 'A) < 1 sec'\n",
    "        WHEN r.avg_duration_ms < 10000 THEN 'B) 1-10 sec'\n",
    "        WHEN r.avg_duration_ms < 60000 THEN 'C) 10-60 sec'\n",
    "        ELSE 'D) > 60 sec'\n",
    "    END AS duration_bin, \n",
    "    count(r) AS num_transitions\n",
    "ORDER BY duration_bin\n",
    "\"\"\"\n",
    "duration_data = run_cypher_query(driver, query_avg_duration_distribution)\n",
    "print(f\"Extracted {len(duration_data)} duration bins.\")\n",
    "\n",
    "\n",
    "# 3. Event Type Popularity\n",
    "print(\"\\n--- Extracting Event Type Popularity ---\")\n",
    "query_event_type_counts = \"\"\"\n",
    "MATCH (e:Event)\n",
    "WHERE e.type IS NOT NULL\n",
    "RETURN e.type AS eventType, sum(e.global_count) AS totalCount\n",
    "ORDER BY totalCount DESC\n",
    "\"\"\"\n",
    "event_type_data = run_cypher_query(driver, query_event_type_counts)\n",
    "print(f\"Extracted {len(event_type_data)} event types.\")\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# --- 2. VISUALIZATIONS ---\n",
    "\n",
    "df_top_events = pd.DataFrame(top_events_data)\n",
    "df_duration = pd.DataFrame(duration_data)\n",
    "df_event_type = pd.DataFrame(event_type_data)\n",
    "\n",
    "\n",
    "# 1. Top 10 Most Popular Events (Bar Chart)\n",
    "print(\"\\nGenerating Top 10 Events Bar Chart...\")\n",
    "fig_events = px.bar(\n",
    "    df_top_events, \n",
    "    x='eventId', \n",
    "    y='count', \n",
    "    title='Top 10 Most Popular Events (Global Count)',\n",
    "    labels={'eventId': 'Event ID', 'count': 'Total Global Count'},\n",
    "    color='count',\n",
    "    color_continuous_scale=px.colors.sequential.Viridis\n",
    ")\n",
    "fig_events.show()\n",
    "\n",
    "\n",
    "# 2. Average Transition Duration Distribution (Bar Chart)\n",
    "print(\"\\nGenerating Transition Duration Distribution...\")\n",
    "fig_duration = px.bar(\n",
    "    df_duration, \n",
    "    x='duration_bin', \n",
    "    y='num_transitions', \n",
    "    title='Distribution of NEXT Transition Durations',\n",
    "    labels={'duration_bin': 'Duration Bin', 'num_transitions': 'Number of Transitions'},\n",
    "    color='num_transitions',\n",
    "    category_orders={\"duration_bin\": [\"A) < 1 sec\", \"B) 1-10 sec\", \"C) 10-60 sec\", \"D) > 60 sec\"]}\n",
    ")\n",
    "fig_duration.show()\n",
    "\n",
    "\n",
    "# 3. Event Type Popularity (Pie Chart)\n",
    "print(\"\\nGenerating Event Type Popularity Pie Chart...\")\n",
    "fig_type = px.pie(\n",
    "    df_event_type, \n",
    "    values='totalCount', \n",
    "    names='eventType', \n",
    "    title='Distribution of Total Event Counts by Event Type',\n",
    "    hole=.3\n",
    ")\n",
    "fig_type.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig_type.show()\n",
    "\n",
    "\n",
    "# 4. Model Performance Comparison (Bar Chart)\n",
    "df_mrr = pd.DataFrame({\n",
    "    'Model': ['Popularity', 'Simple Transition', 'Multi-Hop', 'Temporal Decay'],\n",
    "    'MRR': [mrr_popularity, mrr_transition, mrr_multi_hop, mrr_temporal]\n",
    "})\n",
    "print(\"\\nGenerating Model Performance Comparison...\")\n",
    "fig_mrr = px.bar(\n",
    "    df_mrr,\n",
    "    x='Model',\n",
    "    y='MRR',\n",
    "    title=f'Model MRR@{topk_limit} Comparison',\n",
    "    color='MRR',\n",
    "    color_continuous_scale=px.colors.sequential.Plasma\n",
    ")\n",
    "fig_mrr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aac53b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
