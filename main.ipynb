{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea5a04a",
   "metadata": {},
   "source": [
    "\n",
    "## graph based session intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bb844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/nahomnadew/.cache/kagglehub/datasets/retailrocket/ecommerce-dataset/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"retailrocket/ecommerce-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8665a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial parameters\n",
    "\n",
    "DATA_PATH = \"data/kaggle/events.csv\"  \n",
    "SAMPLE_MAX_SESSIONS = 200000  \n",
    "MIN_SESSION_LENGTH = 2  \n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_AUTH = (\"neo4j\", \"nahi1420\")  \n",
    "BATCH_SIZE = 5000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be81e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 2.3.3\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"pandas\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1fcf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, this may take a while for the full RetailRocket dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8259/3092328460.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(DATA_PATH, parse_dates=['timestamp'], low_memory=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 2756101\n",
      "Columns: ['timestamp', 'visitorid', 'event', 'itemid', 'transactionid']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433221332117</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433224214164</td>\n",
       "      <td>992329</td>\n",
       "      <td>view</td>\n",
       "      <td>248676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433221999827</td>\n",
       "      <td>111016</td>\n",
       "      <td>view</td>\n",
       "      <td>318965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433221955914</td>\n",
       "      <td>483717</td>\n",
       "      <td>view</td>\n",
       "      <td>253185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433221337106</td>\n",
       "      <td>951259</td>\n",
       "      <td>view</td>\n",
       "      <td>367447</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp  visitorid event  itemid  transactionid\n",
       "0  1433221332117     257597  view  355908            NaN\n",
       "1  1433224214164     992329  view  248676            NaN\n",
       "2  1433221999827     111016  view  318965            NaN\n",
       "3  1433221955914     483717  view  253185            NaN\n",
       "4  1433221337106     951259  view  367447            NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset (may be large)\n",
    "assert os.path.exists(DATA_PATH), f\"Data file not found: {DATA_PATH}\"\n",
    "print(\"Loading, this may take a while for the full RetailRocket dataset...\")\n",
    "\n",
    "# The RetailRocket 'events.csv' has columns similar to: sessionId, itemId, eventType, timestamp\n",
    "usecols = None  \n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e887c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed columns mapping: {'visitorid': 'session_id', 'event': 'event_type', 'timestamp': 'timestamp'}\n",
      "After filtering short sessions: 406020 unique sessions, 1754541 rows\n"
     ]
    }
   ],
   "source": [
    "# Corrected RetailRocket column mapping\n",
    "cols = [c.lower() for c in df.columns]\n",
    "colmap = {}\n",
    "if 'visitorid' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='visitorid'][0]] = 'session_id'\n",
    "if 'itemid' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='itemid'][0]] = 'event_id'\n",
    "if 'event' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='event'][0]] = 'event_type'\n",
    "if 'timestamp' in cols:\n",
    "    colmap[[c for c in df.columns if c.lower()=='timestamp'][0]] = 'timestamp'\n",
    "\n",
    "df = df.rename(columns=colmap)\n",
    "print('Renamed columns mapping:', colmap)\n",
    "\n",
    "assert 'session_id' in df.columns and 'event_id' in df.columns and 'timestamp' in df.columns\n",
    "\n",
    "# Add step_index per session (order by timestamp)\n",
    "df = df.sort_values(['session_id','timestamp'])\n",
    "df['step_index'] = df.groupby('session_id').cumcount() + 1\n",
    "\n",
    "# Filter out sessions that are too short\n",
    "session_lengths = df.groupby('session_id').size()\n",
    "valid_sessions = session_lengths[session_lengths >= MIN_SESSION_LENGTH].index\n",
    "df = df[df['session_id'].isin(valid_sessions)].copy()\n",
    "print(\"After filtering short sessions:\", df['session_id'].nunique(), \"unique sessions,\", len(df), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c608219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled sessions: 200000 rows: 869349\n"
     ]
    }
   ],
   "source": [
    "# pick the first N sessions (keeps temporal order inside sessions)\n",
    "if SAMPLE_MAX_SESSIONS is not None:\n",
    "    unique_sess = df['session_id'].drop_duplicates().iloc[:SAMPLE_MAX_SESSIONS]\n",
    "    df = df[df['session_id'].isin(unique_sess)].copy()\n",
    "    print(\"Sampled sessions:\", df['session_id'].nunique(), \"rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe88d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample to data/sampled_sessions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save a sampled CSV for quick checks \n",
    "sample_out = \"data/sampled_sessions.csv\"\n",
    "df.to_csv(sample_out, index=False)\n",
    "print(\"Saved sample to\", sample_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96663298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8259/3748043458.py:1: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n"
     ]
    }
   ],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd87aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j connection helper\n",
    "def get_driver(uri=NEO4J_URI, auth=NEO4J_AUTH):\n",
    "    return GraphDatabase.driver(uri, auth=auth, encrypted=False)\n",
    "\n",
    "def create_constraints(driver):\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(lambda tx: tx.run(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Event) REQUIRE e.id IS UNIQUE\"\"\"))\n",
    "        session.execute_write(lambda tx: tx.run(\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (s:Session) REQUIRE s.id IS UNIQUE\"\"\"))\n",
    "    print('Constraints ensured.')\n",
    "\n",
    "def build_graph_from_df(df, driver, batch_size=BATCH_SIZE):\n",
    "    # df must contain: session_id, timestamp, step_index, event_id, event_type (optional)\n",
    "    create_constraints(driver)\n",
    "    sessions = df['session_id'].drop_duplicates().tolist()\n",
    "    print('Building nodes for', len(sessions), 'sessions (batched)')\n",
    "    with driver.session() as session:\n",
    "        for i in tqdm(range(0, len(sessions), batch_size), desc='sessions'):\n",
    "            batch = sessions[i:i+batch_size]\n",
    "            params = {'sids': batch}\n",
    "            cypher = \"\"\"UNWIND $sids AS sid\n",
    "            MERGE (s:Session {id: sid})\n",
    "            \"\"\"\n",
    "            session.run(cypher, **params)\n",
    "    print('Creating Event nodes and OCCURRED_IN relationships (this may take a while)')\n",
    "    with driver.session() as session:\n",
    "        for sid, group in tqdm(df.groupby('session_id'), desc='sessions_events', total=df['session_id'].nunique()):\n",
    "            g = group.sort_values('step_index')\n",
    "\n",
    "            start_ts = g['timestamp'].iloc[0].isoformat() if not pd.isnull(g['timestamp'].iloc[0]) else None\n",
    "            end_ts = g['timestamp'].iloc[-1].isoformat() if not pd.isnull(g['timestamp'].iloc[-1]) else None\n",
    "            session.run(\"\"\"MATCH (s:Session {id:$sid}) SET s.start_ts=$start_ts, s.end_ts=$end_ts, s.num_events=$num_events\"\"\", sid=sid, start_ts=start_ts, end_ts=end_ts, num_events=len(g))\n",
    "\n",
    "            for r in g.itertuples():\n",
    "                eid = str(r.event_id)\n",
    "                etype = getattr(r, 'event_type', None) if 'event_type' in g.columns else None\n",
    "                session.run(\"\"\"MERGE (e:Event {id:$eid})\n",
    "                ON CREATE SET e.type = $etype, e.global_count = 1\n",
    "                ON MATCH SET e.global_count = coalesce(e.global_count,0) + 1\n",
    "                \"\"\", eid=eid, etype=etype)\n",
    "                session.run(\"\"\"MATCH (e:Event {id:$eid}), (s:Session {id:$sid})\n",
    "                MERGE (e)-[r:OCCURRED_IN {index:$idx, ts:$ts}]->(s)\n",
    "                \"\"\", eid=eid, sid=sid, idx=int(r.step_index), ts=r.timestamp.isoformat() if not pd.isnull(r.timestamp) else None)\n",
    "    print('Event nodes and OCCURRED_IN relationships created.')\n",
    "\n",
    "def build_next_edges(df, driver):\n",
    "\n",
    "    next_counts = Counter()\n",
    "    for sid, group in df.groupby('session_id'):\n",
    "        g = group.sort_values('step_index')\n",
    "        events = list(g['event_id'].astype(str))\n",
    "        for a,b in zip(events, events[1:]):\n",
    "            next_counts[(a,b)] += 1\n",
    "    print('Unique transitions:', len(next_counts))\n",
    "    with driver.session() as session:\n",
    "        for (a,b), cnt in tqdm(next_counts.items(), desc='creating_next'):\n",
    "            session.run(\"\"\"MATCH (a:Event {id:$a}), (b:Event {id:$b})\n",
    "            MERGE (a)-[r:NEXT]->(b)\n",
    "            ON CREATE SET r.count = $cnt\n",
    "            ON MATCH SET r.count = coalesce(r.count,0) + $cnt\n",
    "            \"\"\", a=a, b=b, cnt=int(cnt))\n",
    "    print('NEXT edges created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "094f2120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints ensured.\n",
      "Building nodes for 200000 sessions (batched)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sessions:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sessions: 100%|██████████| 40/40 [00:02<00:00, 17.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Event nodes and OCCURRED_IN relationships (this may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sessions_events: 100%|██████████| 200000/200000 [1:29:49<00:00, 37.11it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event nodes and OCCURRED_IN relationships created.\n",
      "Unique transitions: 433803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating_next: 100%|██████████| 433803/433803 [23:00<00:00, 314.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT edges created.\n",
      "Graph build functions ready. To run: instantiate driver and call build_graph_from_df(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "driver = get_driver()\n",
    "build_graph_from_df(df, driver)\n",
    "build_next_edges(df, driver)\n",
    "driver.close()\n",
    "print('Graph build functions ready. To run: instantiate driver and call build_graph_from_df(...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62e96827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('92233', 1)]\n"
     ]
    }
   ],
   "source": [
    "def recommend_next_transition(driver, last_event_id, topk=10):\n",
    "    q = \"\"\"MATCH (e:Event {id:$last})-[r:NEXT]->(cand:Event)\n",
    "    RETURN cand.id AS id, r.count AS score\n",
    "    ORDER BY r.count DESC\n",
    "    LIMIT $k\n",
    "    \"\"\"\n",
    "    with driver.session() as s:\n",
    "        res = s.run(q, last=last_event_id, k=topk)\n",
    "        return [(r['id'], r['score']) for r in res]\n",
    "\n",
    "# Example usage (uncomment after graph is built):\n",
    "driver = get_driver()\n",
    "print(recommend_next_transition(driver, \"12345\", topk=5))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1b68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
